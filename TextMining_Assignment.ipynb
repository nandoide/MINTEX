{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules importation and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD# SVD = Singular Value Descomposition\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabaz_score, accuracy_score\n",
    "from sklearn.preprocessing import Normalizer, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29606186</td>\n",
       "      <td>Can reactivity and regulation in infancy predi...</td>\n",
       "      <td>A need to identify early infant markers of lat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29471205</td>\n",
       "      <td>Fabrication of bioinspired, self-cleaning supe...</td>\n",
       "      <td>The mechanical properties, corrosion-resistanc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29175165</td>\n",
       "      <td>Functional properties of chickpea protein isol...</td>\n",
       "      <td>In the present study, the effect of Refractanc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29098524</td>\n",
       "      <td>Mechanical dyssynchrony alters left ventricula...</td>\n",
       "      <td>The impact of left bundle branch block (LBBB) ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27507285</td>\n",
       "      <td>Reducing the width of confidence intervals for...</td>\n",
       "      <td>In the last decade, it has been shown that an ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accession number                                              Title  \\\n",
       "0          29606186  Can reactivity and regulation in infancy predi...   \n",
       "1          29471205  Fabrication of bioinspired, self-cleaning supe...   \n",
       "2          29175165  Functional properties of chickpea protein isol...   \n",
       "3          29098524  Mechanical dyssynchrony alters left ventricula...   \n",
       "4          27507285  Reducing the width of confidence intervals for...   \n",
       "\n",
       "                                            Abstract Label  \n",
       "0  A need to identify early infant markers of lat...     0  \n",
       "1  The mechanical properties, corrosion-resistanc...     0  \n",
       "2  In the present study, the effect of Refractanc...     0  \n",
       "3  The impact of left bundle branch block (LBBB) ...     0  \n",
       "4  In the last decade, it has been shown that an ...     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27829177</td>\n",
       "      <td>A naturally occurring variant of HPV-16 E7 exe...</td>\n",
       "      <td>Human Papillomavirus E6 and E7 play critical r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27806271</td>\n",
       "      <td>Functional Analysis of Orai1 Concatemers Suppo...</td>\n",
       "      <td>Store-operated Ca(2+) entry occurs through the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27796307</td>\n",
       "      <td>KAT2A/KAT2B-targeted acetylome reveals a role ...</td>\n",
       "      <td>Lysine acetylation is a widespread post-transl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27795438</td>\n",
       "      <td>The Cellular DNA Helicase ChlR1 Regulates Chro...</td>\n",
       "      <td>In papillomavirus infections, the viral genome...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27794539</td>\n",
       "      <td>Human R1441C LRRK2 regulates the synaptic vesi...</td>\n",
       "      <td>Mutations in leucine-rich repeat kinase 2 (LRR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accession number                                              Title  \\\n",
       "0          27829177  A naturally occurring variant of HPV-16 E7 exe...   \n",
       "1          27806271  Functional Analysis of Orai1 Concatemers Suppo...   \n",
       "2          27796307  KAT2A/KAT2B-targeted acetylome reveals a role ...   \n",
       "3          27795438  The Cellular DNA Helicase ChlR1 Regulates Chro...   \n",
       "4          27794539  Human R1441C LRRK2 regulates the synaptic vesi...   \n",
       "\n",
       "                                            Abstract Label  \n",
       "0  Human Papillomavirus E6 and E7 play critical r...     1  \n",
       "1  Store-operated Ca(2+) entry occurs through the...     1  \n",
       "2  Lysine acetylation is a widespread post-transl...     1  \n",
       "3  In papillomavirus infections, the viral genome...     1  \n",
       "4  Mutations in leucine-rich repeat kinase 2 (LRR...     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29606186</td>\n",
       "      <td>Can reactivity and regulation in infancy predi...</td>\n",
       "      <td>A need to identify early infant markers of lat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29471205</td>\n",
       "      <td>Fabrication of bioinspired, self-cleaning supe...</td>\n",
       "      <td>The mechanical properties, corrosion-resistanc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29175165</td>\n",
       "      <td>Functional properties of chickpea protein isol...</td>\n",
       "      <td>In the present study, the effect of Refractanc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29098524</td>\n",
       "      <td>Mechanical dyssynchrony alters left ventricula...</td>\n",
       "      <td>The impact of left bundle branch block (LBBB) ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27507285</td>\n",
       "      <td>Reducing the width of confidence intervals for...</td>\n",
       "      <td>In the last decade, it has been shown that an ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accession number                                              Title  \\\n",
       "0          29606186  Can reactivity and regulation in infancy predi...   \n",
       "1          29471205  Fabrication of bioinspired, self-cleaning supe...   \n",
       "2          29175165  Functional properties of chickpea protein isol...   \n",
       "3          29098524  Mechanical dyssynchrony alters left ventricula...   \n",
       "4          27507285  Reducing the width of confidence intervals for...   \n",
       "\n",
       "                                            Abstract Label  \n",
       "0  A need to identify early infant markers of lat...     0  \n",
       "1  The mechanical properties, corrosion-resistanc...     0  \n",
       "2  In the present study, the effect of Refractanc...     0  \n",
       "3  The impact of left bundle branch block (LBBB) ...     0  \n",
       "4  In the last decade, it has been shown that an ...     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "#NROWS = sys.maxsize\n",
    "NROWS = 50\n",
    "## Negative dataset\n",
    "df_neg = pd.read_csv('./practica_clase/PRECISION_MEDICINE/negative_training_abstracts.tsv', sep='\\t', \n",
    "                     header=None, nrows = NROWS)\n",
    "\n",
    "df_neg.columns = ['Accession number', 'Title', 'Abstract']\n",
    "df_neg['Label'] = '0' #'neg'\n",
    "\n",
    "display(df_neg.head())\n",
    "\n",
    "corpus_neg = list(df_neg['Abstract'].values)\n",
    "### len(corpus_neg) # 4078\n",
    "\n",
    "## Positive\n",
    "df_pos = pd.read_csv('./practica_clase/PRECISION_MEDICINE/positive_training_abstracts.tsv', sep='\\t', \n",
    "                     header=None, nrows = NROWS)\n",
    "\n",
    "df_pos.columns = ['Accession number', 'Title', 'Abstract']\n",
    "df_pos['Label'] = '1' # 'pos'\n",
    "display(df_pos.head())\n",
    "\n",
    "# Add corpus\n",
    "df_corpus = df_neg.append(df_pos)\n",
    "display(df_corpus.head())\n",
    "\n",
    "# len(corpus) # 8156\n",
    "\n",
    "labels = df_corpus['Label']\n",
    "corpus = df_corpus['Abstract']\n",
    "# len(labels) # 8156\n",
    "\n",
    "print(len(corpus), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    corpus, labels, test_size=TEST_SIZE, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Construction of an automatic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters can be adjusted in order to try to maximize the quality of the\n",
    "classifier:\n",
    "\n",
    "- In function TfidfVectorizer:\n",
    "    * Parameters that affect the vocabulary quality:\n",
    "        * List of stopwords (one of the options is setting it to None)\n",
    "        * maxfeatures\n",
    "        * max_df, min_df\n",
    "    * Norm (none, ‘l1’ or ‘l2’)\n",
    "    \n",
    "- In Latent Semantic Analysis (LSA):\n",
    "    * n_components\n",
    "    * not performing LSA\n",
    "    \n",
    "- Classifier model:\n",
    "    * You can use strategies included in some of the notebooks we used \n",
    "        * Logistic Regression, \n",
    "        * Naïve Bayes, \n",
    "        * decision trees, \n",
    "        * SVC\n",
    "        * or others you learnt from the Machine Learning course (k-nn, neural networks, etc.)\n",
    "\n",
    "The goal is not to check all possible combinations of these parameters but respond to thesequestions:\n",
    "\n",
    "- Which tips can you give about constructing an automatic text classifier? What do you recommend to do? What do you recommend not to do?\n",
    "- What is the best classifier you have obtained?\n",
    "\n",
    "Your responses to these questions should be illustrated with tables and/or figures and/or\n",
    "screen captures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Find additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return words_freq[:n]\n",
    "\n",
    "def improve_stop_words(X_train, n=50):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    common_words = [i[0] for i in get_top_n_words(X_train, n)]\n",
    "    eng_and_custom_stopwords = set(list(stop_words.ENGLISH_STOP_WORDS) + common_words)\n",
    "    print(len(eng_and_custom_stopwords))\n",
    "    return eng_and_custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "CLASSIFIERS = ['knn', 'dtree', 'nb', 'lr', 'svc', 'lsvc']\n",
    "CLASSIFIERS_UNSUPERVISED = ['kmeans']\n",
    "REDUCERS = ['svd', 'kbest', 'percentile', None]\n",
    "CV = 4\n",
    "\n",
    "def create_text_pipeline(reducer='svd', classifier=\"nb\"):\n",
    "    \"\"\" Create text vectorization pipeline with optional dimensionality reduction\"\"\"\n",
    "    assert reducer in REDUCERS, \"ERROR: Reducer %s not supported, only %s\" % (reducer, REDUCERS)\n",
    "    assert classifier in CLASSIFIERS + CLASSIFIERS_UNSUPERVISED,\\\n",
    "        \"ERROR: Classifier %s not supported, only %s\" % (classifier, CLASSIFIERS + CLASSIFIERS_UNSUPERVISED)\n",
    "    pipeline = [\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    "    # Reduce dimensions\n",
    "    if reducer == 'svd':\n",
    "        pipeline.append(('red_svd', TruncatedSVD()))\n",
    "    elif reducer == 'kbest':\n",
    "        pipeline.append(('red_kbest', SelectKBest()))\n",
    "    elif reducer == 'percentile':\n",
    "        pipeline.append(('red_percentile', SelectPercentile()))\n",
    "    elif reducer == None:\n",
    "        pass\n",
    "    \n",
    "    # Classify\n",
    "    if classifier == \"nb\":\n",
    "        if reducer == 'svd':\n",
    "            pipeline.append(('mm_scaler', MinMaxScaler()))\n",
    "        elif reducer == 'kbest':\n",
    "            pipeline.append(('mm_scaler', MaxAbsScaler()))\n",
    "        elif reducer == 'percentile':\n",
    "            pipeline.append(('mm_scaler', MaxAbsScaler()))\n",
    "        elif reducer == None:\n",
    "            pass\n",
    "        pipeline.append(('clf_' + classifier, MultinomialNB()))\n",
    "    elif classifier == \"lr\":\n",
    "        pipeline.append(('clf_' + classifier, LogisticRegression()))\n",
    "    elif classifier == \"svc\":\n",
    "        pipeline.append(('clf_' + classifier, SVC()))\n",
    "    elif classifier == \"lsvc\":\n",
    "        pipeline.append(('clf_' + classifier, LinearSVC()))\n",
    "    elif classifier == \"dtree\":\n",
    "        pipeline.append(('clf_' + classifier, DecisionTreeClassifier())) \n",
    "    elif classifier == \"knn\":\n",
    "        pipeline.append(('clf_' + classifier, KNeighborsClassifier()))    \n",
    "    elif classifier == \"kmeans\":\n",
    "        pipeline.append(('norm', Normalizer()))\n",
    "        pipeline.append(('cluster_kmeans', KMeans()))\n",
    "    elif classifier == None:\n",
    "        pass\n",
    "    \n",
    "    return Pipeline(pipeline)\n",
    "\n",
    "def get_prediction_from_cluster(X, pipeline):\n",
    "    \"\"\" Transform cluster assignment in y_pred object\"\"\"\n",
    "    def swap_label(label):\n",
    "        if label == 1:\n",
    "            return '0'\n",
    "        elif label == 0:\n",
    "            return '1'\n",
    "        else:\n",
    "            return str(label)\n",
    "    labels = pipeline.predict(X_test)\n",
    "    labels_predicted = [str(label) for label in labels]\n",
    "    predicted = pd.Series(labels_predicted)\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    labels_predicted_reverse = [swap_label(label) for label in labels]\n",
    "    predicted_reverse = pd.Series(labels_predicted_reverse)\n",
    "    accuracy_reverse = metrics.accuracy_score(y_test, predicted_reverse)\n",
    "    if accuracy_reverse > accuracy: predicted = predicted_reverse\n",
    "    return predicted\n",
    "\n",
    "def get_filtered_params(parameters, pipeline):\n",
    "    \"\"\" Filter the params that aren't related to steps in the pipeline \"\"\"\n",
    "    filtered_params = {}\n",
    "    for param_key in parameters.keys():\n",
    "        if param_key.split('__')[0] in pipeline.named_steps.keys():\n",
    "            filtered_params[param_key] = parameters[param_key]\n",
    "    return filtered_params\n",
    "\n",
    "def get_filtered_set(parameters, pipeline):\n",
    "    \"\"\" Filter the params that aren't related to steps in the pipeline \"\"\"\n",
    "    if type(parameters) == dict:\n",
    "        return get_filtered_params(parameters, pipeline)\n",
    "    else:\n",
    "        filtered_set = []\n",
    "        for param_set in parameters:\n",
    "            filtered_set.append(get_filtered_params(param_set, pipeline))\n",
    "        return filtered_set\n",
    "    \n",
    "def prediction_metrics(X_train, y_train, X_test, y_test, parameters, reducer=\"svd\", classifier=\"nb\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"### Reducer: %s   Classifier: %s\" %(reducer, classifier))\n",
    "    pipeline = create_text_pipeline(reducer=reducer, classifier=classifier)\n",
    "    pipeline.set_params(**get_filtered_params(parameters, pipeline))\n",
    "    print(\"Pipeline\", pipeline.named_steps)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    if classifier in CLASSIFIERS_UNSUPERVISED:\n",
    "        predicted = get_prediction_from_cluster(X_test, pipeline)\n",
    "    else:\n",
    "        predicted = pipeline.predict(X_test)\n",
    "    print()\n",
    "    print(\"Accuracy\", metrics.accuracy_score(y_test, predicted))\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "    \n",
    "def process_classifications(X_train, y_train, X_test, y_test, parameters, \n",
    "                            classifiers=CLASSIFIERS, reducers=REDUCERS):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for classifier in classifiers:\n",
    "            for reducer in reducers:\n",
    "                prediction_metrics(X_train, y_train, X_test, y_test, parameters, reducer, classifier)\n",
    "\n",
    "\n",
    "def prediction_metrics_grid(X_train, y_train, X_test, y_test, parameters_grid, reducer=\"svd\", classifier=\"nb\", cv=CV):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"### Reducer: %s   Classifier: %s\" %(reducer, classifier))\n",
    "    pipeline = create_text_pipeline(reducer=reducer, classifier=classifier)\n",
    "    filtered_params = get_filtered_set(parameters_grid, pipeline)\n",
    "    #scoring = {'accuracy': make_scorer(accuracy_score), 'calinski': make_scorer(calinski_harabaz_score)}\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score)}\n",
    "    grid_model = GridSearchCV(pipeline, filtered_params, cv=cv, iid=False, error_score=0,\n",
    "                              scoring=None, refit=False)\n",
    "    grid_model.fit(X_train, y_train)\n",
    "    print()\n",
    "    print(\"Best parameters\")\n",
    "    for param_name in sorted(grid_model.best_params_.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, grid_model.best_params_[param_name]))  \n",
    "    pipeline.set_params(**grid_model.best_params_)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    if classifier in CLASSIFIERS_UNSUPERVISED:\n",
    "        predicted = get_prediction_from_cluster(X_test, pipeline)\n",
    "    else:\n",
    "        predicted = pipeline.predict(X_test)\n",
    "    print()\n",
    "    print(\"Accuracy\", metrics.accuracy_score(y_test, predicted))\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "    \n",
    "def process_classifications_grid(X_train, y_train, X_test, y_test, parameters, cv=CV,\n",
    "                            classifiers=CLASSIFIERS, reducers=REDUCERS):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for classifier in classifiers:\n",
    "            for reducer in reducers:\n",
    "                prediction_metrics_grid(X_train, y_train, X_test, y_test, parameters, reducer, classifier, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main process with prefixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Reducer: svd   Classifier: knn\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'clf_knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "           weights='uniform')}\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       1.00      0.89      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[15  0]\n",
      " [ 2 16]]\n",
      "### Reducer: kbest   Classifier: knn\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "           weights='uniform')}\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[14  1]\n",
      " [ 2 16]]\n",
      "### Reducer: percentile   Classifier: knn\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "           weights='uniform')}\n",
      "\n",
      "Accuracy 0.696969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.74        15\n",
      "           1       0.90      0.50      0.64        18\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        33\n",
      "   macro avg       0.75      0.72      0.69        33\n",
      "weighted avg       0.77      0.70      0.69        33\n",
      "\n",
      "[[14  1]\n",
      " [ 9  9]]\n",
      "### Reducer: None   Classifier: knn\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "           weights='uniform')}\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       1.00      0.89      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[15  0]\n",
      " [ 2 16]]\n",
      "### Reducer: svd   Classifier: dtree\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'clf_dtree': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')}\n",
      "\n",
      "Accuracy 0.8787878787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88        15\n",
      "           1       1.00      0.78      0.88        18\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        33\n",
      "   macro avg       0.89      0.89      0.88        33\n",
      "weighted avg       0.90      0.88      0.88        33\n",
      "\n",
      "[[15  0]\n",
      " [ 4 14]]\n",
      "### Reducer: kbest   Classifier: dtree\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_dtree': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')}\n",
      "\n",
      "Accuracy 0.9696969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.95      1.00      0.97        18\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        33\n",
      "   macro avg       0.97      0.97      0.97        33\n",
      "weighted avg       0.97      0.97      0.97        33\n",
      "\n",
      "[[14  1]\n",
      " [ 0 18]]\n",
      "### Reducer: percentile   Classifier: dtree\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_dtree': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')}\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: None   Classifier: dtree\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_dtree': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: svd   Classifier: nb\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'mm_scaler': MinMaxScaler(copy=True, feature_range=(0, 1)), 'clf_nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        15\n",
      "           1       1.00      0.83      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.92      0.92      0.91        33\n",
      "weighted avg       0.92      0.91      0.91        33\n",
      "\n",
      "[[15  0]\n",
      " [ 3 15]]\n",
      "### Reducer: kbest   Classifier: nb\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'mm_scaler': MaxAbsScaler(copy=True), 'clf_nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}\n",
      "\n",
      "Accuracy 0.5454545454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        15\n",
      "           1       1.00      0.17      0.29        18\n",
      "\n",
      "   micro avg       0.55      0.55      0.55        33\n",
      "   macro avg       0.75      0.58      0.48        33\n",
      "weighted avg       0.77      0.55      0.46        33\n",
      "\n",
      "[[15  0]\n",
      " [15  3]]\n",
      "### Reducer: percentile   Classifier: nb\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'mm_scaler': MaxAbsScaler(copy=True), 'clf_nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n",
      "### Reducer: None   Classifier: nb\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_nb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n",
      "### Reducer: svd   Classifier: lr\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'clf_lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)}\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: kbest   Classifier: lr\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)}\n",
      "\n",
      "Accuracy 0.7575757575757576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.93      0.78        15\n",
      "           1       0.92      0.61      0.73        18\n",
      "\n",
      "   micro avg       0.76      0.76      0.76        33\n",
      "   macro avg       0.79      0.77      0.76        33\n",
      "weighted avg       0.80      0.76      0.75        33\n",
      "\n",
      "[[14  1]\n",
      " [ 7 11]]\n",
      "### Reducer: percentile   Classifier: lr\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)}\n",
      "\n",
      "Accuracy 0.8484848484848485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        15\n",
      "           1       0.93      0.78      0.85        18\n",
      "\n",
      "   micro avg       0.85      0.85      0.85        33\n",
      "   macro avg       0.86      0.86      0.85        33\n",
      "weighted avg       0.86      0.85      0.85        33\n",
      "\n",
      "[[14  1]\n",
      " [ 4 14]]\n",
      "### Reducer: None   Classifier: lr\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: svd   Classifier: svc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'clf_svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)}\n",
      "\n",
      "Accuracy 0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        15\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        33\n",
      "   macro avg       0.23      0.50      0.31        33\n",
      "weighted avg       0.21      0.45      0.28        33\n",
      "\n",
      "[[15  0]\n",
      " [18  0]]\n",
      "### Reducer: kbest   Classifier: svc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)}\n",
      "\n",
      "Accuracy 0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        15\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        33\n",
      "   macro avg       0.23      0.50      0.31        33\n",
      "weighted avg       0.21      0.45      0.28        33\n",
      "\n",
      "[[15  0]\n",
      " [18  0]]\n",
      "### Reducer: percentile   Classifier: svc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)}\n",
      "\n",
      "Accuracy 0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        15\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        33\n",
      "   macro avg       0.23      0.50      0.31        33\n",
      "weighted avg       0.21      0.45      0.28        33\n",
      "\n",
      "[[15  0]\n",
      " [18  0]]\n",
      "### Reducer: None   Classifier: svc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_svc': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)}\n",
      "\n",
      "Accuracy 0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62        15\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        33\n",
      "   macro avg       0.23      0.50      0.31        33\n",
      "weighted avg       0.21      0.45      0.28        33\n",
      "\n",
      "[[15  0]\n",
      " [18  0]]\n",
      "### Reducer: svd   Classifier: lsvc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=40, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'clf_lsvc': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)}\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: kbest   Classifier: lsvc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_kbest': SelectKBest(k=3, score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_lsvc': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)}\n",
      "\n",
      "Accuracy 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.93      0.82        15\n",
      "           1       0.93      0.72      0.81        18\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        33\n",
      "   macro avg       0.83      0.83      0.82        33\n",
      "weighted avg       0.84      0.82      0.82        33\n",
      "\n",
      "[[14  1]\n",
      " [ 5 13]]\n",
      "### Reducer: percentile   Classifier: lsvc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_percentile': SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x1a1b98f1e0>), 'clf_lsvc': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)}\n",
      "\n",
      "Accuracy 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81        15\n",
      "           1       0.88      0.78      0.82        18\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        33\n",
      "   macro avg       0.82      0.82      0.82        33\n",
      "weighted avg       0.82      0.82      0.82        33\n",
      "\n",
      "[[13  2]\n",
      " [ 4 14]]\n",
      "### Reducer: None   Classifier: lsvc\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=6,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'clf_lsvc': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)}\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n"
     ]
    }
   ],
   "source": [
    "# First set of parameters\n",
    "param_set_1 = { \n",
    "    'vect__norm': None,\n",
    "    'vect__smooth_idf': True,\n",
    "    'vect__sublinear_tf': True,\n",
    "    'vect__max_features': 1000,\n",
    "    'vect__min_df': 6,\n",
    "    'vect__stop_words': 'english',\n",
    "    'vect__strip_accents' : 'unicode',\n",
    "    'vect__analyzer' : 'word',\n",
    "    'vect__token_pattern': r'\\w{1,}', \n",
    "    'vect__ngram_range' : (1, 2),\n",
    "    'scaler' : None,\n",
    "    'red_kbest__k' : 3,\n",
    "    'red_percentile__score_func' : f_classif,\n",
    "    'red_percentile__percentile' : 10,\n",
    "    #'scaler__with_mean' : False,\n",
    "    'vect__norm': 'l2',\n",
    "    'red_svd__n_components': 40,\n",
    "    'clf_knn__n_neighbors' : 2,\n",
    "}\n",
    "\n",
    "# More stop words\n",
    "#eng_and_custom_stopwords = improve_stop_words(X_train, 200)\n",
    "#param_set_1['vect__stop_words'] = eng_and_custom_stopwords\n",
    "\n",
    "process_classifications(X_train, y_train, X_test, y_test, param_set_1, reducers=REDUCERS, classifiers=CLASSIFIERS)\n",
    "\n",
    "#process_classifications(X_train, y_train, X_test, y_test, param_set_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main process with grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "### Reducer: svd   Classifier: knn\n",
      "\n",
      "Best parameters\n",
      "\tclf_knn__n_neighbors: 2\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 5.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8787878787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        15\n",
      "           1       0.89      0.89      0.89        18\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        33\n",
      "   macro avg       0.88      0.88      0.88        33\n",
      "weighted avg       0.88      0.88      0.88        33\n",
      "\n",
      "[[13  2]\n",
      " [ 2 16]]\n",
      "### Reducer: kbest   Classifier: knn\n",
      "\n",
      "Best parameters\n",
      "\tclf_knn__n_neighbors: 5\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 6\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: percentile   Classifier: knn\n",
      "\n",
      "Best parameters\n",
      "\tclf_knn__n_neighbors: 5\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.93      0.82        15\n",
      "           1       0.93      0.72      0.81        18\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        33\n",
      "   macro avg       0.83      0.83      0.82        33\n",
      "weighted avg       0.84      0.82      0.82        33\n",
      "\n",
      "[[14  1]\n",
      " [ 5 13]]\n",
      "### Reducer: None   Classifier: knn\n",
      "\n",
      "Best parameters\n",
      "\tclf_knn__n_neighbors: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8484848484848485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        15\n",
      "           1       0.93      0.78      0.85        18\n",
      "\n",
      "   micro avg       0.85      0.85      0.85        33\n",
      "   macro avg       0.86      0.86      0.85        33\n",
      "weighted avg       0.86      0.85      0.85        33\n",
      "\n",
      "[[14  1]\n",
      " [ 4 14]]\n",
      "### Reducer: svd   Classifier: dtree\n",
      "\n",
      "Best parameters\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[14  1]\n",
      " [ 2 16]]\n",
      "### Reducer: kbest   Classifier: dtree\n",
      "\n",
      "Best parameters\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[14  1]\n",
      " [ 2 16]]\n",
      "### Reducer: percentile   Classifier: dtree\n",
      "\n",
      "Best parameters\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: None   Classifier: dtree\n",
      "\n",
      "Best parameters\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        15\n",
      "           1       0.86      1.00      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.93      0.90      0.91        33\n",
      "weighted avg       0.92      0.91      0.91        33\n",
      "\n",
      "[[12  3]\n",
      " [ 0 18]]\n",
      "### Reducer: svd   Classifier: nb\n",
      "\n",
      "Best parameters\n",
      "\tred_svd__n_components: 40\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[14  1]\n",
      " [ 2 16]]\n",
      "### Reducer: kbest   Classifier: nb\n",
      "\n",
      "Best parameters\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.5151515151515151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65        15\n",
      "           1       1.00      0.11      0.20        18\n",
      "\n",
      "   micro avg       0.52      0.52      0.52        33\n",
      "   macro avg       0.74      0.56      0.43        33\n",
      "weighted avg       0.77      0.52      0.41        33\n",
      "\n",
      "[[15  0]\n",
      " [16  2]]\n",
      "### Reducer: percentile   Classifier: nb\n",
      "\n",
      "Best parameters\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8787878787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        15\n",
      "           1       0.85      0.94      0.89        18\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        33\n",
      "   macro avg       0.89      0.87      0.88        33\n",
      "weighted avg       0.88      0.88      0.88        33\n",
      "\n",
      "[[12  3]\n",
      " [ 1 17]]\n",
      "### Reducer: None   Classifier: nb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 6\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n",
      "### Reducer: svd   Classifier: lr\n",
      "\n",
      "Best parameters\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[14  1]\n",
      " [ 2 16]]\n",
      "### Reducer: kbest   Classifier: lr\n",
      "\n",
      "Best parameters\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: percentile   Classifier: lr\n",
      "\n",
      "Best parameters\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: None   Classifier: lr\n",
      "\n",
      "Best parameters\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: svd   Classifier: svc\n",
      "\n",
      "Best parameters\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 1000\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: kbest   Classifier: svc\n",
      "\n",
      "Best parameters\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: percentile   Classifier: svc\n",
      "\n",
      "Best parameters\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "### Reducer: None   Classifier: svc\n",
      "\n",
      "Best parameters\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 6\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9696969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.95      1.00      0.97        18\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        33\n",
      "   macro avg       0.97      0.97      0.97        33\n",
      "weighted avg       0.97      0.97      0.97        33\n",
      "\n",
      "[[14  1]\n",
      " [ 0 18]]\n",
      "### Reducer: svd   Classifier: lsvc\n",
      "\n",
      "Best parameters\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9393939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n",
      "[[14  1]\n",
      " [ 1 17]]\n",
      "### Reducer: kbest   Classifier: lsvc\n",
      "\n",
      "Best parameters\n",
      "\tred_kbest__k: 3\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: None\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8484848484848485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        15\n",
      "           1       0.93      0.78      0.85        18\n",
      "\n",
      "   micro avg       0.85      0.85      0.85        33\n",
      "   macro avg       0.86      0.86      0.85        33\n",
      "weighted avg       0.86      0.85      0.85        33\n",
      "\n",
      "[[14  1]\n",
      " [ 4 14]]\n",
      "### Reducer: percentile   Classifier: lsvc\n",
      "\n",
      "Best parameters\n",
      "\tred_percentile__percentile: 10\n",
      "\tred_percentile__score_func: <function f_classif at 0x1a1b98f1e0>\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 6\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l2'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n",
      "### Reducer: None   Classifier: lsvc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 900\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: 'english'\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        33\n",
      "   macro avg       0.91      0.91      0.91        33\n",
      "weighted avg       0.91      0.91      0.91        33\n",
      "\n",
      "[[13  2]\n",
      " [ 1 17]]\n"
     ]
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'vect__norm': ['l1', 'l2', None],\n",
    "    'vect__smooth_idf': [True],\n",
    "    'vect__sublinear_tf': [True],\n",
    "    'vect__max_features': [900, 1000],\n",
    "    'vect__min_df': [1, 5, 6],\n",
    "    'vect__max_df': [1., 5., 6],\n",
    "    'vect__stop_words': [None, 'english', eng_and_custom_stopwords],\n",
    "    'vect__strip_accents' : ['unicode'],\n",
    "    'vect__analyzer' : ['word'],\n",
    "    'vect__token_pattern': [r'\\w{1,}'], \n",
    "    'vect__ngram_range' : [(1, 2)],\n",
    "    'scaler' : [None],\n",
    "    'red_svd__n_components': [2, 30, 40],\n",
    "    'clf_knn__n_neighbors' : [2, 5],\n",
    "    'red_percentile__score_func' : [f_classif],\n",
    "    'red_percentile__percentile' : [10],\n",
    "    'red_kbest__k' : [3]  \n",
    "}\n",
    "\n",
    "eng_and_custom_stopwords = improve_stop_words(X_train, 200)\n",
    "#prediction_metrics_grid(X_train, y_train, X_test, y_test, parameters_grid, reducer='svd', classifier=\"knn\", cv=2)\n",
    "process_classifications_grid(X_train, y_train, X_test, y_test, parameters_grid, cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Construction of a clustering of biology documents\n",
    "We already know the class information in our dataset (positive and negative) but we will test if an automatic clustering system discovers automatically these classes (“labels”). The objective is to learn strategies that will be very useful when we have to cluster unlabeled documents. Therefore, we “hide” this information (the real class) to the clustering algorithm. \n",
    "\n",
    "The objective in this section is to check what are the parameters that maximize clustering’s quality. The parameters to be taken into account are:\n",
    "\n",
    "- In function TfidfVectorizer:\n",
    "\n",
    "    * Vocabulary (larger or smaller)\n",
    "    * Norm (none, ‘l1’ or ‘l2’)\n",
    "    \n",
    "- In Latent Semantic Analysis (LSA):\n",
    "\n",
    "    * n_components\n",
    "    * o not performing LSA\n",
    "    \n",
    "- Normalize the data/not normalize it with “Normalizer” (included in the notebook).\n",
    "\n",
    "The questions to be responded in this part are:\n",
    "\n",
    "- Which tips can you give about constructing a text clustering with k-means? What do you recommend to do? What do you recommend not to do?\n",
    "\n",
    "- What is the best clustering you have obtained? The quality of the cluster is the degree of correspondence between real class and assigned cluster. For example:\n",
    "\n",
    "    * If there are 2 clusters and cluster 0 contains all examples of positive class and cluster 1 contains all examples of negative class, the clustering is perfect.\n",
    "    * If there are 2 clusters and cluster 1 contains all examples of positive class and cluster 0 contains all examples of negative class, the clustering is also perfect.\n",
    "    *  If there are 2 clusters and cluster 0 contains 50% of examples of positive class and 50% of examples of negative class, and statistics in cluster 1 are similar, the clustering quality is the worst possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main process with prefixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Reducer: svd   Classifier: kmeans\n",
      "Pipeline {'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'scaler': None, 'red_svd': TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'norm': None, 'cluster_kmeans': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)}\n",
      "\n",
      "Accuracy 0.8787878787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.85        15\n",
      "           1       0.82      1.00      0.90        18\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        33\n",
      "   macro avg       0.91      0.87      0.87        33\n",
      "weighted avg       0.90      0.88      0.88        33\n",
      "\n",
      "[[11  4]\n",
      " [ 0 18]]\n"
     ]
    }
   ],
   "source": [
    "param_set_1 = { \n",
    "    'vect__smooth_idf': True,\n",
    "    'vect__sublinear_tf': True,\n",
    "    'vect__max_features': 1000,\n",
    "    'vect__min_df': 1,\n",
    "    'vect__max_df': 1.,\n",
    "    'vect__stop_words': 'english',\n",
    "    'vect__strip_accents' : 'unicode',\n",
    "    'vect__analyzer' : 'word',\n",
    "    'vect__token_pattern': r'\\w{1,}', \n",
    "    'vect__ngram_range' : (1, 2),\n",
    "    #'scaler__with_mean' : False,\n",
    "    'vect__norm': 'l2',\n",
    "    'red_svd__n_components': 100,\n",
    "    'clf_knn__n_neighbors' : 2,\n",
    "    'cluster_kmeans__n_clusters' : 2,\n",
    "    'red_kbest__k' : 3,\n",
    "    'red_percentile__score_func' : f_classif,\n",
    "    'red_percentile__percentile' : 10,\n",
    "    'scaler': None,\n",
    "    'norm': None\n",
    "}\n",
    "\n",
    "process_classifications(X_train, y_train, X_test, y_test, param_set_1, reducers=['svd'], classifiers=['kmeans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main process with grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "462\n",
      "### Reducer: svd   Classifier: kmeans\n",
      "\n",
      "Best parameters\n",
      "\tcluster_kmeans__n_clusters: 2\n",
      "\tnorm: None\n",
      "\tred_svd__n_components: 2\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 30\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.7878787878787878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.70        15\n",
      "           1       0.72      1.00      0.84        18\n",
      "\n",
      "   micro avg       0.79      0.79      0.79        33\n",
      "   macro avg       0.86      0.77      0.77        33\n",
      "weighted avg       0.85      0.79      0.77        33\n",
      "\n",
      "[[ 8  7]\n",
      " [ 0 18]]\n",
      "### Reducer: kbest   Classifier: kmeans\n",
      "\n",
      "Best parameters\n",
      "\tcluster_kmeans__n_clusters: 2\n",
      "\tnorm: None\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 30\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77        15\n",
      "           1       0.77      0.94      0.85        18\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        33\n",
      "   macro avg       0.84      0.81      0.81        33\n",
      "weighted avg       0.83      0.82      0.81        33\n",
      "\n",
      "[[10  5]\n",
      " [ 1 17]]\n",
      "### Reducer: percentile   Classifier: kmeans\n",
      "\n",
      "Best parameters\n",
      "\tcluster_kmeans__n_clusters: 2\n",
      "\tnorm: None\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 20\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.696969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.40      0.55        15\n",
      "           1       0.65      0.94      0.77        18\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        33\n",
      "   macro avg       0.76      0.67      0.66        33\n",
      "weighted avg       0.75      0.70      0.67        33\n",
      "\n",
      "[[ 6  9]\n",
      " [ 1 17]]\n",
      "### Reducer: None   Classifier: kmeans\n",
      "\n",
      "Best parameters\n",
      "\tcluster_kmeans__n_clusters: 2\n",
      "\tnorm: None\n",
      "\tscaler: None\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: 20\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__norm: 'l1'\n",
      "\tvect__smooth_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: 'unicode'\n",
      "\tvect__sublinear_tf: True\n",
      "\tvect__token_pattern: '\\\\w{1,}'\n",
      "\n",
      "Accuracy 0.7575757575757576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.53      0.67        15\n",
      "           1       0.71      0.94      0.81        18\n",
      "\n",
      "   micro avg       0.76      0.76      0.76        33\n",
      "   macro avg       0.80      0.74      0.74        33\n",
      "weighted avg       0.79      0.76      0.74        33\n",
      "\n",
      "[[ 8  7]\n",
      " [ 1 17]]\n"
     ]
    }
   ],
   "source": [
    "eng_and_custom_stopwords = improve_stop_words(X_train, 200)\n",
    "parameters_grid = [\n",
    "    {'vect__norm': ['l1', 'l2', None],\n",
    "    'vect__smooth_idf': [True],\n",
    "    'vect__sublinear_tf': [True],\n",
    "    'vect__max_features': [20, 30],\n",
    "    'vect__min_df': [1, 5],\n",
    "    'vect__max_df': [1., 6],\n",
    "    'vect__stop_words': [None, 'english', eng_and_custom_stopwords],\n",
    "    'vect__strip_accents' : ['unicode'],\n",
    "    'vect__analyzer' : ['word'],\n",
    "    'vect__token_pattern': [r'\\w{1,}'], \n",
    "    'vect__ngram_range' : [(1, 2)],\n",
    "    'scaler' : [None],\n",
    "    'red_svd__n_components': [2, 10, 15],\n",
    "    'clf_knn__n_neighbors' : [2, 5],\n",
    "    'cluster_kmeans__n_clusters' : [2],\n",
    "    'norm' : [None]},\n",
    "    # without svd\n",
    "    {'vect__norm': ['l1', 'l2', None],\n",
    "    'vect__smooth_idf': [True],\n",
    "    'vect__sublinear_tf': [True],\n",
    "    'vect__max_features': [20, 30],\n",
    "    'vect__min_df': [1, 5],\n",
    "    'vect__max_df': [1., 6],\n",
    "    'vect__stop_words': [None, 'english', eng_and_custom_stopwords],\n",
    "    'vect__strip_accents' : ['unicode'],\n",
    "    'vect__analyzer' : ['word'],\n",
    "    'vect__token_pattern': [r'\\w{1,}'], \n",
    "    'vect__ngram_range' : [(1, 2)],\n",
    "    'scaler' : [None],\n",
    "    'red_svd__n_components': [2, 10, 15],\n",
    "    'clf_knn__n_neighbors' : [2, 5],\n",
    "    'cluster_kmeans__n_clusters' : [2],\n",
    "    'red__svd' : [None]}\n",
    "]\n",
    "\n",
    "eng_and_custom_stopwords = improve_stop_words(X_train, 200)\n",
    "#prediction_metrics_grid(X_train, y_train, X_test, y_test, parameters_grid, reducer=\"reducer\", classifier=\"kmeans\", cv=CV)\n",
    "process_classifications_grid(X_train, y_train, X_test, y_test, parameters_grid, classifiers=[\"kmeans\"], cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 1000)\n",
      "Evaluando k=1\n",
      "Evaluando k=2\n",
      "Evaluando k=3\n",
      "Evaluando k=4\n",
      "Evaluando k=5\n",
      "Evaluando k=6\n",
      "Evaluando k=7\n",
      "Evaluando k=8\n",
      "Evaluando k=9\n",
      "Evaluando k=10\n",
      "Evaluando k=11\n",
      "Evaluando k=12\n",
      "Evaluando k=13\n",
      "Evaluando k=14\n",
      "Evaluando k=15\n",
      "Evaluando k=16\n",
      "Evaluando k=17\n",
      "Evaluando k=18\n",
      "Evaluando k=19\n",
      "Evaluando k=20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def get_X_transform(X):\n",
    "    vectorizador = TfidfVectorizer(max_df=1., max_features=1000, norm='l2',\n",
    "                                   min_df=1, stop_words='english',\n",
    "                                   #stop_words=stopwords,\n",
    "                                   #token_pattern=r'(?u)\\b[A-Za-z]+\\b',\n",
    "                                   #token_pattern=r'(?ui)\\b\\w*[a-z]+\\w*\\b',\n",
    "                                   use_idf=True)\n",
    "    \n",
    "    vectorizador = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
    "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
    "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
    "        vocabulary=None)\n",
    "    \n",
    "    X = vectorizador.fit_transform(X)\n",
    "\n",
    "    print(X.shape)\n",
    "    n_componentes = 100\n",
    "    svd_truncado = TruncatedSVD(n_componentes)\n",
    "    normalizador = Normalizer(copy=False)\n",
    "\n",
    "    lsa = make_pipeline(svd_truncado, normalizador)\n",
    "    #lsa = svd_truncado\n",
    "\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "    varianza_explicada = svd_truncado.explained_variance_ratio_.sum()\n",
    "    normalizer = Normalizer()\n",
    "    X_lsa_norm = normalizer.fit_transform(X_lsa)\n",
    "    return X_lsa_norm\n",
    "\n",
    "X_km = get_X_transform(X_train)\n",
    "qmetric = calinski_harabaz_score\n",
    "\n",
    "Nclusters_max = 15\n",
    "Nrepetitions = 100\n",
    "\n",
    "qualities = []\n",
    "inertias = []\n",
    "models = []\n",
    "kini = 1\n",
    "kfin = 20\n",
    "for k in range(kini,kfin+1):\n",
    "    print(\"Evaluando k=%d\" % k)\n",
    "    km = KMeans(n_clusters=k,\n",
    "                init='k-means++', n_init=Nrepetitions,\n",
    "                max_iter=500, random_state=2)    \n",
    "    km.fit(X_km)\n",
    "    models.append(km)\n",
    "    inertias.append(km.inertia_)\n",
    "    if k > 1:\n",
    "        qualities.append(qmetric(X_km, km.labels_))\n",
    "        #qualities.append(km.score(X_km))\n",
    "    else:\n",
    "        qualities.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of clusters 1\n",
      "Number of clusters of best quality 2\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14,3))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.plot(range(kini,kfin+1), inertias, marker='o')\n",
    "plt.xlabel('number of clusters')\n",
    "plt.title('clustering inertia')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.plot(range(kini,kfin+1), qualities, marker='o')\n",
    "plt.xlabel('number of clusters')\n",
    "plt.title('clustering quality')\n",
    "plt.show()\n",
    "\n",
    "best = pd.Series(qualities).idxmax() # get index for the best model\n",
    "print(\"Best number of clusters\", best)\n",
    "km = models[best]\n",
    "n_clusters = km.get_params()['n_clusters']\n",
    "clusters = km.labels_\n",
    "print ('Number of clusters of best quality', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 1000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.90      1.00      0.95        18\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        33\n",
      "   macro avg       0.95      0.93      0.94        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "[[13  2]\n",
      " [ 0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.13      0.11        15\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.06      0.06      0.06        33\n",
      "   macro avg       0.05      0.07      0.06        33\n",
      "weighted avg       0.05      0.06      0.05        33\n",
      "\n",
      "[[ 2 13]\n",
      " [18  0]]\n"
     ]
    }
   ],
   "source": [
    "# We choose the best option to evaluate the quality of prediction\n",
    "X = X_test\n",
    "y = y_test\n",
    "X_km = get_X_transform(X)\n",
    "labels = km.fit_predict(X_km)\n",
    "#print(labels)\n",
    "# First we try with labels as is \n",
    "labels_predicted = [str(label) for label in labels]\n",
    "predicted = pd.Series(labels_predicted)\n",
    "#print(labels_predicted)\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "# Alternatively we invert the label to match the real labels of each group\n",
    "labels_predicted = [str((label + 1)%2) for label in labels]\n",
    "#print(labels_predicted)\n",
    "predicted = pd.Series(labels_predicted)\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_metadata": {
     "hidden": "true",
     "lexer": "bash"
    }
   },
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "latex_metadata": {
     "hidden": "true",
     "lexer": "bash"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert --to=latex --template=~/report.tplx TextMining_Assignment.ipynb 1>/dev/null 2>/dev/null\n",
    "/Library/TeX/texbin/pdflatex -shell-escape TextMining_Assignment 1>/dev/null 2>/dev/null\n",
    "jupyter nbconvert --to html_toc TextMining_Assignment.ipynb 1>/dev/null 2>/dev/null"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "latex_metadata": {
   "author": "Elena Montenegro, Fernando Freire",
   "title": "Text Mining Assignment"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.828125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
